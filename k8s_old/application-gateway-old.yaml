# 2. HealthCheckPolicy: Gateway API를 위한 상태 확인 설정 리소스
# "상태 확인은 GRPC로 하라"고 명시
# https://cloud.google.com/kubernetes-engine/docs/how-to/configure-gateway-resources#configure_health_check
apiVersion: networking.gke.io/v1
kind: HealthCheckPolicy
metadata:
  name: vac-hub-grpc-health-check-policy
  namespace: grpc-test
spec:
  # 이 정책이 적용될 대상을 명시적으로 지정합니다.
  targetRef:
    group: ""
    kind: Service
    name: vac-hub-test-svc
  default:
    checkIntervalSec: 15
    healthyThreshold: 1
    unhealthyThreshold: 2
    config:
      type: GRPC
      grpcHealthCheck:
        port: 50051
---
# 3. GCPBackendPolicy: "클라이언트 연결 후 10분간 데이터가 없어도 끊지 마"
apiVersion: networking.gke.io/v1
kind: GCPBackendPolicy
metadata:
  name: vac-hub-timeout-policy
  namespace: grpc-test
spec:
  # 정책이 적용될 Service를 명시적으로 지정합니다.
  targetRef:
    group: ""
    kind: Service
    name: vac-hub-test-svc
  default:
    # 유휴 연결 타임아웃 (길게 설정)
    timeoutSec: 600
---        
# 4. 애플리케이션 Service (ClusterIP)
# HealthCheckPolicy를 어노테이션으로 연결
apiVersion: v1
kind: Service
metadata:
  name: vac-hub-test-svc
  namespace: grpc-test
  annotations:
    # Gateway API가 Pod를 직접 타겟팅(NEG)하도록 설정
    cloud.google.com/neg: '{"gateway": true}'
spec:
  type: ClusterIP
  selector:
    app: vac-hub-test
  ports:
  - name: grpc
    protocol: TCP
    port: 50051
    targetPort: 50051
    # ADDED: 이 포트가 gRPC 프로토콜을 사용함을 명시적으로 알려줍니다.
    # appProtocol: GRPC
    appProtocol: kubernetes.io/h2c
---
# 5. Kubernetes Gateway: GKE에 Cloud Load Balancer 생성을 요청합니다.
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: vac-hub-gateway
  namespace: grpc-test
spec:
  # 표준 GKE L7 로드밸런서 클래스를 사용합니다.
  #gatewayClassName: gke-l7-gxlb
  gatewayClassName: gke-l7-global-external-managed  
  listeners:
  - name: https
    protocol: HTTPS
    port: 443
    allowedRoutes:
      namespaces:
        from: Same
    tls:
      mode: Terminate # 로드밸런서에서 TLS 종료
      certificateRefs:
      - name: grpc-cert # 로컬에서 생성한 TLS Secret
        kind: Secret # 참조하는 리소스의 종류
        group: ""
---
# 6. HTTPRoute: Gateway로 들어온 트래픽을 서비스로 라우팅합니다.
# gRPC는 HTTP/2 기반이므로 HTTPRoute로 처리가능합니다.
# GCPBackendPolicy(타임아웃용)를 필터로 연결
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: vac-hub-http-route
  namespace: grpc-test
spec:  
  parentRefs:
  - kind: Gateway
    name: vac-hub-gateway
  hostnames:
  - "grpc.example.com"
#    sectionName: https
  rules:
  - backendRefs:
    - name: vac-hub-test-svc
      port: 50051
---
# --- 신규 추가: OpenTelemetry Collector 설정 (ConfigMap) ---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-conf
  namespace: grpc-test
data:
  otel-collector-config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
    processors:
      batch:
      # ### 이것이 최종 해결책입니다 ###
      # Collector가 실행 중인 GKE 환경 정보를 감지하도록 설정합니다.
      # 이 프로세서는 파드 이름, 네임스페이스, 라벨 등의 '리소스 속성'을 자동으로 추가합니다.
      resourcedetection/gke:
        detectors: [gke]

    exporters:
      googlemanagedprometheus: {}
      logging:
        loglevel: debug
    service:
      pipelines:
        metrics:
          # 파이프라인에 resourcedetection 프로세서를 추가합니다.
          # 순서: OTLP로 받고 -> 리소스 정보 추가 -> 배치 처리 -> 내보내기
          receivers: [otlp]
          processors: [resourcedetection/gke, batch]
          exporters: [googlemanagedprometheus, logging]
---
# --- 신규 추가: OTEL Collector를 위한 Kubernetes 서비스 계정(KSA) ---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector-sa # 이 이름은 gcloud 명령어에서 사용한 KSA_NAME과 일치해야 합니다.
  namespace: grpc-test
  # ### 가장 중요한 수정 ###
  # 이 어노테이션이 KSA와 GSA를 연결합니다.
  annotations:
    iam.gke.io/gcp-service-account: otel-collector-sa@${PROJECT_ID}.iam.gserviceaccount.com
---
# --- 신규 추가: OpenTelemetry Collector 서비스 ---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: grpc-test
spec:
  selector:
    app: otel-collector
  ports:
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
  - name: otlp-http
    port: 4318
    targetPort: 4318
---
# --- 신규 추가: OpenTelemetry Collector 배포 ---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: grpc-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      # ### 가장 중요한 수정 ###
      # 파드가 위에서 생성한 KSA를 사용하도록 지정합니다.
      serviceAccountName: otel-collector-sa
      containers:
      - name: otel-collector
        # 권한 문제가 해결되었으므로, 공식 권장 이미지를 사용합니다.
        image: gcr.io/google-opentelemetry/prometheus-engine/opentelemetry-collector:latest
        command:
        - "/otelcol"
        - "--config=/conf/otel-collector-config.yaml"
        ports:
        - name: otlp-grpc
          containerPort: 4317
        - name: prometheus
          containerPort: 8888
        volumeMounts:
        - name: otel-collector-config-vol
          mountPath: /conf
      volumes:
      - name: otel-collector-config-vol
        configMap:
          name: otel-collector-conf
---
# --- 수정: gRPC 앱 배포 ---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vac-hub-test
  namespace: grpc-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vac-hub-test
  template:
    metadata:
      labels:
        app: vac-hub-test
    spec:
      terminationGracePeriodSeconds: 60
      containers:
      - name: vac-hub-test-server
        image: "${REGION}-docker.pkg.dev/${PROJECT_ID}/grpc-test-repo/vac-hub-test:${IMAGE_TAG}"
        # 환경 변수를 통해 Collector 서비스 주소를 전달합니다.
        env:
        - name: OTEL_COLLECTOR_ENDPOINT
          value: "otel-collector.grpc-test.svc.cluster.local:4317"
        ports:
        - containerPort: 50051
          name: grpc
        # --- Prometheus 포트 8000은 더 이상 필요 없습니다 ---
        readinessProbe:
          grpc:
            port: 50051
          initialDelaySeconds: 5
---
# HPA (변경 없음. 최종 수정된 External 타입 유지)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vac-hub-test-hpa
  namespace: grpc-test
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vac-hub-test
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: External
    external:
      metric:
        name: prometheus.googleapis.com|grpc_server_active_calls_gauge|gauge
        selector:
          matchLabels:
            metric.labels.app: vac-hub-test
      target:
        type: AverageValue
        averageValue: "3"
---
# --- 수정: PodMonitoring ---
# https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed#gmp-pod-monitoring
apiVersion: monitoring.googleapis.com/v1
kind: PodMonitoring
metadata:
  name: otel-collector-pod-monitoring # 이름 변경 (더 명확하게)
  namespace: grpc-test
spec:
  # ### 가장 중요한 수정 ###
  # 이제 gRPC 앱이 아닌, otel-collector 파드를 감시합니다.
  selector:
    matchLabels:
      app: otel-collector
  endpoints:
  # Collector는 8888 포트에서 Prometheus 형식의 메트릭을 노출합니다.
  - port: prometheus # Collector Deployment의 포트 이름과 일치
    path: /metrics
    interval: 30s